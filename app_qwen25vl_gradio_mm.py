# app_qwen25vl_gradio_auto.py
# è‡ªåŠ¨è¯†åˆ«ï¼šä¼˜å…ˆè§†é¢‘ > å›¾ç‰‡ > çº¯èŠå¤©
# ä¾èµ–ï¼špip install gradio pillow transformers torch
#       ï¼ˆè§†é¢‘åˆ†æéœ€ï¼špip install opencv-pythonï¼‰

import os
import gc
import glob
from typing import List, Optional, Tuple

os.environ.setdefault("TOKENIZERS_PARALLELISM", "false")
# ï¼ˆå¯é€‰ï¼‰é™åˆ¶å¯è§ GPUï¼ˆåœ¨ import å‰è®¾ç½®æ›´ç¨³ï¼‰
# os.environ["CUDA_VISIBLE_DEVICES"] = "0,1,2,3"   # NVIDIA
# os.environ["HIP_VISIBLE_DEVICES"]  = "0,1,2,3"   # æµ·å…‰/ROCm

import torch
from PIL import Image
import gradio as gr
from transformers import AutoModelForVision2Seq, AutoProcessor   # â† æ”¹ä¸º Vision2Seq

# OpenCVï¼ˆä»…è§†é¢‘æ¨¡å¼éœ€è¦ï¼‰
try:
    import cv2
except Exception:
    cv2 = None

# ===== åŸºç¡€é…ç½® =====
MODEL_DIR = os.path.abspath("./Qwen2.5-VL-7B-Instruct")

if torch.cuda.is_available():
    TORCH_DTYPE = torch.float16
    DEVICE_MAP = "auto"
    try:
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.set_float32_matmul_precision("high")
    except Exception:
        pass
else:
    TORCH_DTYPE = torch.float32       # CPU å›é€€æ›´ç¨³
    DEVICE_MAP = "auto"

# ===== æ¨¡å‹åŠ è½½ï¼ˆä¼˜å…ˆ flash-attn2ï¼Œå¤±è´¥å›é€€ sdpaï¼‰=====
def _load_with_attn(attn_impl: Optional[str]):
    kw = dict(
        trust_remote_code=True,
        torch_dtype=TORCH_DTYPE,
        device_map=DEVICE_MAP,
        low_cpu_mem_usage=True,
        # local_files_only=True,  # å®Œå…¨ç¦»çº¿å¯å¼€å¯
    )
    if attn_impl:
        kw["attn_implementation"] = attn_impl
    # å…³é”®ï¼šä½¿ç”¨ Vision2Seq
    return AutoModelForVision2Seq.from_pretrained(MODEL_DIR, **kw)

def load_model_and_processor():
    try:
        model = _load_with_attn("flash_attention_2")
        print("[info] ä½¿ç”¨ flash_attention_2")
    except Exception as e:
        print(f"[warn] flash_attention_2 ä¸å¯ç”¨ï¼Œå›é€€ sdpaï¼š{e}")
        model = _load_with_attn("sdpa")

    processor = AutoProcessor.from_pretrained(
        MODEL_DIR,
        trust_remote_code=True,
        # local_files_only=True,
    )
    return model, processor

MODEL, PROCESSOR = load_model_and_processor()

# ===== å·¥å…·å‡½æ•°ï¼šå›¾ç‰‡æ–‡ä»¶åˆ—è¡¨ & åŠ è½½ =====
IMG_EXTS = (".png", ".jpg", ".jpeg", ".bmp", ".webp")

def list_local_images(search_dir="."):
    files = []
    for ext in IMG_EXTS:
        files.extend(glob.glob(os.path.join(search_dir, f"*{ext}")))
    files = [os.path.relpath(p, start=search_dir) for p in files]
    files.sort()
    return files

def load_image_from_path(path_str: str) -> Tuple[Optional[Image.Image], str]:
    if not path_str:
        return None, "è¯·è¾“å…¥å›¾ç‰‡æœ¬åœ°è·¯å¾„"
    path = os.path.expanduser(path_str)
    if not os.path.isabs(path):
        path = os.path.abspath(path)
    if not os.path.exists(path):
        return None, f"æ–‡ä»¶ä¸å­˜åœ¨ï¼š{path}"
    try:
        im = Image.open(path).convert("RGB")
        return im, f"å·²åŠ è½½ï¼š{path}"
    except Exception as e:
        return None, f"åŠ è½½å¤±è´¥ï¼š{e}"

def load_image_from_dropdown(name: str) -> Tuple[Optional[Image.Image], str]:
    if not name:
        return None, "è¯·å…ˆä»ä¸‹æ‹‰æ¡†é€‰æ‹©å›¾ç‰‡"
    path = os.path.abspath(name)
    return load_image_from_path(path)

# ===== è§†é¢‘æŠ½å¸§ =====
def sample_video_frames_cv2(video_path: str, num_frames: int = 16) -> List[Image.Image]:
    if cv2 is None:
        raise RuntimeError("æœªå®‰è£… opencv-pythonï¼Œè¯·å…ˆï¼špip install opencv-python")
    if not os.path.exists(video_path):
        raise FileNotFoundError(f"è§†é¢‘ä¸å­˜åœ¨ï¼š{video_path}")
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        raise RuntimeError(f"æ— æ³•æ‰“å¼€è§†é¢‘ï¼š{video_path}")

    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT)) or 0
    if frame_count == 0:
        cap.release()
        raise RuntimeError("è§†é¢‘å¸§æ•°ä¸º 0")

    idxs = list({int(i) for i in [round(k * (frame_count - 1) / max(1, num_frames - 1)) for k in range(num_frames)]})
    idxs.sort()
    images: List[Image.Image] = []
    for idx in idxs:
        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
        ok, frame = cap.read()
        if not ok:
            continue
        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        images.append(Image.fromarray(frame).convert("RGB"))
    cap.release()
    if not images:
        raise RuntimeError("æœªæˆåŠŸæŠ½å–åˆ°å¸§")
    return images

# ===== æ„é€ å¤šæ¨¡æ€æ¶ˆæ¯ =====
def build_messages_text(user_text: str):
    return [{"role": "user", "content": [{"type": "text", "text": user_text}]}]

def build_messages_image(image: Image.Image, user_text: Optional[str]):
    return [{
        "role": "user",
        "content": [
            {"type": "image", "image": image},
            {"type": "text", "text": user_text or "è¯·ç”¨ä¸­æ–‡è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡çš„ä¸»è¦å†…å®¹ã€å…³é”®ç‰©ä½“ä¸åœºæ™¯ä¿¡æ¯ã€‚"}
        ]
    }]

def build_messages_video(frames: List[Image.Image], user_text: Optional[str]):
    # Qwen2.5-VL æ”¯æŒå°†å¤šå¸§ä½œä¸ºä¸€ä¸ª video æ®µä¼ å…¥
    return [{
        "role": "user",
        "content": [
            {"type": "video", "video": frames},
            {"type": "text", "text": user_text or "è¯·ç”¨ä¸­æ–‡æ¦‚è¿°è¯¥è§†é¢‘çš„ä¸»è¦å†…å®¹ã€åœºæ™¯å˜åŒ–ä¸å…³é”®åŠ¨ä½œã€‚"}
        ]
    }]

# ===== é€šç”¨æ¨ç† =====
def infer(messages, max_new_tokens=256, temperature=0.7, top_p=0.9):
    prompt = PROCESSOR.apply_chat_template(messages, add_generation_prompt=True)

    images = None
    videos = None
    # åªå–æœ¬è½®çš„å›¾/è§†é¢‘ï¼ˆå¦‚éœ€æ”¯æŒå¤šæ®µï¼Œå¯æ‰©å±•ä¸ºç´¯ç§¯åˆ—è¡¨ï¼‰
    for m in messages:
        for c in m.get("content", []):
            if c.get("type") == "image":
                images = [c["image"]]
            elif c.get("type") == "video":
                videos = [c["video"]]

    inputs = PROCESSOR(text=[prompt], images=images, videos=videos, return_tensors="pt")
    # å°è¯•é€åˆ°æ¨¡å‹è®¾å¤‡ï¼›device_map=auto æ—¶ä¸å¼ºæ±‚
    try:
        inputs = inputs.to(MODEL.device)
    except Exception:
        pass

    with torch.no_grad():
        out_ids = MODEL.generate(
            **inputs,
            max_new_tokens=int(max_new_tokens),
            do_sample=True,
            temperature=float(temperature),
            top_p=float(top_p),
        )
    out_text = PROCESSOR.batch_decode(out_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0].strip()

    # ç®€å•æŠ“å–æœ€åä¸€ä¸ª assistant æ®µ
    answer = out_text
    for tok in ["\nassistant\n", "\nassistant:\n", "assistant\n", "assistant:"][::-1]:
        if tok in out_text:
            answer = out_text.split(tok)[-1].strip()
            break
    return answer

# ===== Gradio UIï¼ˆè‡ªåŠ¨è¯†åˆ«æ¨¡å¼ï¼‰=====
with gr.Blocks(title="Qwen2.5-VL è‡ªåŠ¨æ¨¡å¼ï¼ˆèŠå¤©/å›¾ç‰‡/è§†é¢‘ï¼‰", css="footer {display:none!important;}") as demo:
    gr.Markdown(
        """
        # Qwen2.5-VL-7B-Instruct å¤šæ¨¡æ€æ¼”ç¤ºï¼ˆè‡ªåŠ¨è¯†åˆ«ï¼šè§†é¢‘ > å›¾ç‰‡ > çº¯èŠå¤©ï¼‰
        - æ¨¡å‹ç›®å½•ï¼š`./Qwen2.5-VL-7B-Instruct`
        - ä¼˜å…ˆä½¿ç”¨ **flash_attention_2**ï¼›ä¸å¯ç”¨æ—¶è‡ªåŠ¨å›é€€ **sdpa**ã€‚
        """
    )

    with gr.Row():
        with gr.Column(scale=1):
            # æ–‡æœ¬æç¤ºï¼šæ‰€æœ‰æ¨¡å¼éƒ½ä¼šç”¨ä½œâ€œæŒ‡ä»¤â€
            text_in = gr.Textbox(label="æ–‡æœ¬æŒ‡ä»¤ï¼ˆå¯é€‰ï¼šç”¨äºçº¯èŠ/å›¾ç‰‡/è§†é¢‘çš„æç¤ºè¯ï¼‰", lines=3,
                                 placeholder="åœ¨æ­¤è¾“å…¥ä½ çš„é—®é¢˜æˆ–æŒ‡ä»¤")

            # å›¾ç‰‡ä¸‰ç§æ–¹å¼ï¼šä¸Šä¼  / æœ¬åœ°è·¯å¾„ / ä¸‹æ‹‰
            image_in = gr.Image(type="pil", label="ä¸Šä¼ å›¾ç‰‡ï¼ˆå¯é€‰ï¼‰")
            with gr.Row():
                path_tb = gr.Textbox(label="æœ¬åœ°å›¾ç‰‡è·¯å¾„ï¼ˆå¯é€‰ï¼‰", placeholder="/abs/path/to/img.png æˆ– ./å›¾ç‰‡1.png")
                load_from_path_btn = gr.Button("åŠ è½½è·¯å¾„å›¾ç‰‡", variant="secondary")
            path_status = gr.Markdown("")
            with gr.Row():
                dropdown = gr.Dropdown(choices=list_local_images("."), label="å½“å‰ç›®å½•å›¾ç‰‡ï¼ˆå¯é€‰ï¼‰", allow_custom_value=False)
                refresh_btn = gr.Button("åˆ·æ–°åˆ—è¡¨", variant="secondary")
                load_from_dd_btn = gr.Button("åŠ è½½é€‰ä¸­å›¾ç‰‡", variant="secondary")
            dd_status = gr.Markdown("")

            # è§†é¢‘ï¼šä¸Šä¼ æˆ–è·¯å¾„
            video_in = gr.Video(label="ä¸Šä¼ è§†é¢‘ï¼ˆå¯é€‰ï¼‰")
            with gr.Row():
                video_path_tb = gr.Textbox(label="æœ¬åœ°è§†é¢‘è·¯å¾„ï¼ˆå¯é€‰ï¼‰", placeholder="/abs/path/to/video.mp4 æˆ– ./a.mp4")
                video_path_btn = gr.Button("ç¡®è®¤è§†é¢‘è·¯å¾„", variant="secondary")
            video_status = gr.Markdown("")
            num_frames = gr.Slider(4, 64, value=16, step=2, label="è§†é¢‘æŠ½å¸§æ•°é‡ï¼ˆé»˜è®¤ 16ï¼‰")

            with gr.Accordion("é«˜çº§ç”Ÿæˆå‚æ•°", open=False):
                temperature = gr.Slider(0.0, 1.5, value=0.7, step=0.05, label="temperature")
                top_p = gr.Slider(0.1, 1.0, value=0.9, step=0.05, label="top_p")
                max_tokens = gr.Slider(16, 1024, value=256, step=8, label="max_new_tokens")

            gen_btn = gr.Button("ğŸš€ ç”Ÿæˆï¼ˆè‡ªåŠ¨è¯†åˆ«æ¨¡å¼ï¼‰", variant="primary")

        with gr.Column(scale=1):
            chat = gr.Chatbot(label="å¯¹è¯çª—å£ï¼ˆä¿ç•™æœ€è¿‘ 6 è½®ï¼‰", height=560)
            clear_btn = gr.ClearButton([image_in, text_in, chat, path_tb, dropdown, video_in, video_path_tb],
                                       value="æ¸…ç©º")

    # å›¾ç‰‡ï¼šè·¯å¾„ & ä¸‹æ‹‰
    load_from_path_btn.click(lambda p: load_image_from_path(p), inputs=[path_tb], outputs=[image_in, path_status])
    refresh_btn.click(lambda: gr.update(choices=list_local_images(".")), inputs=[], outputs=[dropdown])
    load_from_dd_btn.click(lambda n: load_image_from_dropdown(n), inputs=[dropdown], outputs=[image_in, dd_status])

    # è§†é¢‘ï¼šä»…ç¡®è®¤è·¯å¾„å­˜åœ¨æ€§ï¼ˆçœŸæ­£æŠ½å¸§åœ¨æ¨ç†æ—¶åšï¼‰
    def _confirm_video_path(p):
        if not p:
            return "è¯·è¾“å…¥è§†é¢‘æœ¬åœ°è·¯å¾„ï¼ˆå¯é€‰ï¼‰"
        path = os.path.abspath(os.path.expanduser(p))
        return f"å·²è®¾ç½®è§†é¢‘è·¯å¾„ï¼š{path}ï¼ˆå°†åœ¨æ¨ç†æ—¶å°è¯•è¯»å–å¹¶æŠ½å¸§ï¼‰"
    video_path_btn.click(_confirm_video_path, inputs=[video_path_tb], outputs=[video_status])

    # â€”â€” è‡ªåŠ¨è¯†åˆ« + å¤šè½® â€”â€” #
    def _infer(txt, img, vid, img_path, dd_name, vid_path, frames, hist, t, p, mx):
        """
        è‡ªåŠ¨åˆ¤åˆ«ä¼˜å…ˆçº§ï¼šè§†é¢‘ > å›¾ç‰‡ > çº¯èŠå¤©
        vid: gr.Video è¿”å›çš„å¯¹è±¡ï¼ˆå¯èƒ½æ˜¯ä¸´æ—¶æ–‡ä»¶è·¯å¾„æˆ– dict å¸¦ nameï¼‰
        """

        # æ±‡å…¥å†å²
        MAX_HISTORY = 6
        messages = []
        for u, a in hist[-MAX_HISTORY:]:
            if u:
                messages.append({"role": "user", "content": [{"type": "text", "text": u}]})
            if a:
                messages.append({"role": "assistant", "content": [{"type": "text", "text": a}]})

        # è§£æè§†é¢‘è¾“å…¥
        video_file = None
        if isinstance(vid, dict) and "name" in vid and vid["name"]:
            video_file = vid["name"]
        elif isinstance(vid, str) and vid:
            video_file = vid
        if not video_file and vid_path:
            video_file = os.path.abspath(os.path.expanduser(vid_path))

        # è‹¥æœ‰è§†é¢‘ â†’ è§†é¢‘åˆ†æ
        if video_file and os.path.exists(video_file):
            try:
                frames_list = sample_video_frames_cv2(video_file, num_frames=int(frames))
            except Exception as e:
                return hist + [("(è§†é¢‘)", f"æŠ½å¸§å¤±è´¥ï¼š{e}")]
            messages.extend(build_messages_video(frames_list, (txt or "").strip() or None))
            answer = infer(messages, max_new_tokens=mx, temperature=t, top_p=p)
            hist = hist + [(txt if txt else "(è§†é¢‘åˆ†æ)", answer)]
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            gc.collect()
            return hist

        # è§£æå›¾ç‰‡è¾“å…¥ï¼šä¼˜å…ˆå·²ä¸Šä¼  â†’ è·¯å¾„ â†’ ä¸‹æ‹‰
        image_obj = img
        if image_obj is None and img_path:
            image_obj, _ = load_image_from_path(img_path)
        if image_obj is None and dd_name:
            image_obj, _ = load_image_from_dropdown(dd_name)

        if image_obj is not None:
            messages.extend(build_messages_image(image_obj, (txt or "").strip() or None))
            answer = infer(messages, max_new_tokens=mx, temperature=t, top_p=p)
            hist = hist + [(txt if txt else "(å›¾ç‰‡åˆ†æ)", answer)]
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            gc.collect()
            return hist

        # å¦åˆ™èµ°çº¯èŠå¤©
        user_text = (txt or "").strip()
        if not user_text:
            user_text = "æˆ‘ä»¬æ¥èŠèŠï¼šä½ æ”¯æŒå“ªäº›èƒ½åŠ›ï¼Ÿ"
        messages.extend(build_messages_text(user_text))
        answer = infer(messages, max_new_tokens=mx, temperature=t, top_p=p)
        hist = hist + [(user_text, answer)]
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()
        return hist

    gen_btn.click(
        fn=_infer,
        inputs=[text_in, image_in, video_in, path_tb, dropdown, video_path_tb,
                num_frames, chat, temperature, top_p, max_tokens],
        outputs=[chat]
    )

demo.queue(max_size=32).launch(server_name="0.0.0.0", server_port=7860, inbrowser=False)
