# app_qwen25vl_gradio.py
import os
import gc
import glob
import torch
from PIL import Image
import gradio as gr
from transformers import AutoModelForVision2Seq, AutoProcessor  # â† æ”¹æˆ Vision2Seq

# â€”â€” åŸºç¡€ç¯å¢ƒè®¾ç½® â€”â€”
os.environ.setdefault("TOKENIZERS_PARALLELISM", "false")

MODEL_DIR = os.path.abspath("./Qwen2.5-VL-7B-Instruct")

# ======= è®¾å¤‡/ç²¾åº¦ =======
if torch.cuda.is_available():
    TORCH_DTYPE = torch.float16
    DEVICE_MAP = "auto"
    try:
        torch.backends.cuda.matmul.allow_tf32 = True
        torch.set_float32_matmul_precision("high")
    except Exception:
        pass
else:
    # CPU æˆ–å…¶ä»–è®¾å¤‡å›é€€
    TORCH_DTYPE = torch.float32
    DEVICE_MAP = "auto"

# ======= æ¨¡å‹åŠ è½½ =======
def load_model_and_processor():
    model = AutoModelForVision2Seq.from_pretrained(   # â† æ”¹æˆ Vision2Seq
        MODEL_DIR,
        trust_remote_code=True,
        torch_dtype=TORCH_DTYPE,
        device_map=DEVICE_MAP,
        low_cpu_mem_usage=True,
        # local_files_only=True,  # å®Œå…¨ç¦»çº¿å¯å¼€å¯
        # attn_implementation="flash_attention_2",  # å¯é€‰ï¼šå·²è£…FA2æ—¶å¯å¼€
    )
    processor = AutoProcessor.from_pretrained(
        MODEL_DIR,
        trust_remote_code=True,
        # local_files_only=True,
    )
    return model, processor

MODEL, PROCESSOR = load_model_and_processor()

# ======= å·¥å…·å‡½æ•° =======
IMG_EXTS = (".png", ".jpg", ".jpeg", ".bmp", ".webp")

def list_local_images(search_dir="."):
    files = []
    for ext in IMG_EXTS:
        files.extend(glob.glob(os.path.join(search_dir, f"*{ext}")))
    files = [os.path.relpath(p, start=search_dir) for p in files]
    files.sort()
    return files

def load_image_from_path(path_str):
    if not path_str:
        return None, "è¯·è¾“å…¥å›¾ç‰‡æœ¬åœ°è·¯å¾„"
    path = os.path.expanduser(path_str)
    if not os.path.isabs(path):
        path = os.path.abspath(path)
    if not os.path.exists(path):
        return None, f"æ–‡ä»¶ä¸å­˜åœ¨ï¼š{path}"
    try:
        im = Image.open(path).convert("RGB")
        return im, f"å·²åŠ è½½ï¼š{path}"
    except Exception as e:
        return None, f"åŠ è½½å¤±è´¥ï¼š{e}"

def load_image_from_dropdown(name):
    if not name:
        return None, "è¯·å…ˆä»ä¸‹æ‹‰æ¡†é€‰æ‹©å›¾ç‰‡"
    path = os.path.abspath(name)
    return load_image_from_path(path)

# ======= æ¨ç†ï¼ˆæ”¯æŒå¤šè½®ï¼‰ =======
def generate_reply(image: Image.Image, user_text: str, history, t=0.7, p=0.9, mx=256):
    if image is None and not user_text.strip():
        return history

    MAX_HISTORY = 6
    messages = []

    # æ³¨å…¥æœ€è¿‘çš„å†å²ï¼ˆä»…æ–‡æœ¬å¾€æ¥ï¼‰
    for u, a in history[-MAX_HISTORY:]:
        if u:
            messages.append({"role": "user", "content": [{"type": "text", "text": u}]})
        if a:
            messages.append({"role": "assistant", "content": [{"type": "text", "text": a}]})

    # æœ¬è½®è¾“å…¥
    content = []
    if image is not None:
        content.append({"type": "image", "image": image})
    if user_text.strip():
        content.append({"type": "text", "text": user_text.strip()})
    else:
        content.append({"type": "text", "text": "è¯·ç”¨ä¸­æ–‡è¯¦ç»†æè¿°è¿™å¼ å›¾ç‰‡çš„ä¸»è¦å†…å®¹ã€å…³é”®ç‰©ä½“ä¸åœºæ™¯ä¿¡æ¯ã€‚"})
    messages.append({"role": "user", "content": content})

    # ç¼–ç ä¸ç”Ÿæˆï¼ˆä¸æµ‹è¯•æ ·ä¾‹ä¸€è‡´çš„èŒƒå¼ï¼‰
    prompt = PROCESSOR.apply_chat_template(messages, add_generation_prompt=True)
    inputs = PROCESSOR(
        text=[prompt],
        images=[image] if image is not None else None,  # çº¯æ–‡æœ¬è½®æ¬¡æ—¶ä¼  None
        return_tensors="pt"
    )

    # å…¼å®¹ device_map="auto" çš„åˆ†å¸ƒå¼æƒé‡ï¼šå°½é‡å°†è¾“å…¥ç§»åˆ°ç¬¬ä¸€ä¸ªå¯ç”¨è®¾å¤‡
    try:
        inputs = inputs.to(MODEL.device)  # å¤§å¤šæ•°æƒ…å†µä¸‹å¯ç”¨
    except Exception:
        # é€€è€Œæ±‚å…¶æ¬¡ï¼šä¸å¼ºæ±‚ .to(MODEL.device)
        pass

    with torch.no_grad():
        output_ids = MODEL.generate(
            **inputs,
            max_new_tokens=int(mx),
            do_sample=True,
            temperature=float(t),
            top_p=float(p),
        )

    output_text = PROCESSOR.batch_decode(
        output_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False
    )[0].strip()

    # å°½é‡æŠ½å–æœ€åä¸€æ®µ assistant å›å¤ï¼›è‹¥æ— æ ‡è®°åˆ™ç›´æ¥ç”¨å…¨æ–‡
    answer = output_text
    for tok in ["\nassistant\n", "\nassistant:\n", "assistant\n", "assistant:"][::-1]:
        if tok in output_text:
            answer = output_text.split(tok)[-1].strip()
            break

    history = history + [(user_text if user_text else "(å›¾ç‰‡æè¿°)", answer)]
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    gc.collect()
    return history

# ======= Gradio UI =======
with gr.Blocks(title="Qwen2.5-VL æœ¬åœ°å¤šæ¨¡æ€ï¼ˆGradioï¼‰", css="footer {display: none !important;}") as demo:
    gr.Markdown(
        """
        # Qwen2.5-VL-7B-Instruct æœ¬åœ°å¤šæ¨¡æ€æ¼”ç¤º
        - æ¨¡å‹ï¼š`./Qwen2.5-VL-7B-Instruct`ï¼ˆæœ¬åœ°åŠ è½½ï¼‰
        - **ç”¨æ³•**ï¼šâ‘  é¼ æ ‡ä¸Šä¼ å›¾ç‰‡ï¼ˆå·¦ä¾§ï¼‰ï¼›â‘¡ è¾“å…¥æœ¬åœ°è·¯å¾„åŠ è½½ï¼›â‘¢ ä¸‹æ‹‰æ¡†ä¸€é”®é€‰æ‹©å½“å‰ç›®å½•å›¾ç‰‡ã€‚
        - å³ä¾§èŠå¤©çª—å£æ”¯æŒ**å¤šè½®å¯¹è¯**ï¼ˆä¿ç•™æœ€è¿‘ 6 è½®ï¼‰ã€‚
        """
    )

    with gr.Row():
        with gr.Column(scale=1):
            # æ–¹å¼ â‘ ï¼šä¸Šä¼ å›¾ç‰‡
            image_in = gr.Image(type="pil", label="ä¸Šä¼ å›¾ç‰‡ï¼ˆæ–¹å¼â‘ ï¼‰")

            # æ–¹å¼ â‘¡ï¼šæ–‡æœ¬è·¯å¾„åŠ è½½
            with gr.Row():
                path_tb = gr.Textbox(label="æœ¬åœ°å›¾ç‰‡è·¯å¾„ï¼ˆæ–¹å¼â‘¡ï¼‰", placeholder="/abs/path/to/å›¾ç‰‡.png æˆ– ./å›¾ç‰‡1.png")
                load_from_path_btn = gr.Button("åŠ è½½è¯¥è·¯å¾„å›¾ç‰‡", variant="secondary")
            path_status = gr.Markdown("")

            # æ–¹å¼ â‘¢ï¼šä¸‹æ‹‰é€‰æ‹©å½“å‰ç›®å½•å›¾ç‰‡
            with gr.Row():
                dropdown = gr.Dropdown(choices=list_local_images("."), label="å½“å‰ç›®å½•å›¾ç‰‡ï¼ˆæ–¹å¼â‘¢ï¼‰", allow_custom_value=False)
                refresh_btn = gr.Button("åˆ·æ–°åˆ—è¡¨", variant="secondary")
                load_from_dd_btn = gr.Button("åŠ è½½é€‰ä¸­å›¾ç‰‡", variant="secondary")
            dd_status = gr.Markdown("")

            # æ–‡æœ¬è¾“å…¥
            text_in = gr.Textbox(label="æ–‡æœ¬æŒ‡ä»¤ï¼ˆç•™ç©º=è‡ªåŠ¨åšä¸­æ–‡å›¾ç‰‡æè¿°ï¼‰", lines=3)

            with gr.Accordion("é«˜çº§å‚æ•°", open=False):
                temperature = gr.Slider(0.0, 1.5, value=0.7, step=0.05, label="temperature")
                top_p = gr.Slider(0.1, 1.0, value=0.9, step=0.05, label="top_p")
                max_tokens = gr.Slider(16, 1024, value=256, step=8, label="max_new_tokens")

            gen_btn = gr.Button("ğŸš€ ç”Ÿæˆå›å¤", variant="primary")

        with gr.Column(scale=1):
            chat = gr.Chatbot(label="å¯¹è¯çª—å£", height=560)
            clear_btn = gr.ClearButton([image_in, text_in, chat, path_tb], value="æ¸…ç©º")

    # äº¤äº’ï¼šåŠ è½½å›¾ç‰‡ï¼ˆæ–¹å¼â‘¡ï¼šè·¯å¾„ï¼‰
    def _load_path(path_str):
        im, msg = load_image_from_path(path_str)
        return im, msg

    load_from_path_btn.click(
        fn=_load_path,
        inputs=[path_tb],
        outputs=[image_in, path_status]
    )

    # äº¤äº’ï¼šåˆ·æ–°ä¸‹æ‹‰æ¡†ï¼ˆæ–¹å¼â‘¢ï¼‰
    def _refresh_dd():
        return gr.update(choices=list_local_images("."))

    refresh_btn.click(
        fn=_refresh_dd,
        inputs=[],
        outputs=[dropdown]
    )

    # äº¤äº’ï¼šåŠ è½½ä¸‹æ‹‰æ¡†é€‰ä¸­å›¾ç‰‡ï¼ˆæ–¹å¼â‘¢ï¼‰
    def _load_dd(name):
        im, msg = load_image_from_dropdown(name)
        return im, msg

    load_from_dd_btn.click(
        fn=_load_dd,
        inputs=[dropdown],
        outputs=[image_in, dd_status]
    )

    # ç”Ÿæˆå›å¤ï¼ˆå¤šè½®ï¼‰
    def _infer(img, txt, hist, t, p, mx):
        return generate_reply(img, txt, hist, t, p, mx)

    gen_btn.click(
        fn=_infer,
        inputs=[image_in, text_in, chat, temperature, top_p, max_tokens],
        outputs=[chat]
    )

demo.queue(max_size=32).launch(server_name="0.0.0.0", server_port=7860, inbrowser=False)
